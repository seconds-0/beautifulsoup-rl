# PRIME-RL single-file config for BeautifulSoup RL environment.
# Intended to run via:  uv run prime-rl @ configs/prime-rl/beautiful-soup-env.toml
#
# This follows the official "single TOML" prime-rl layout shown in Verifiers docs.
#
# Tune as needed:
# - [model].name (pick your base model)
# - [orchestrator] batch_size / rollouts_per_example (based on GPU + throughput)
# - [[orchestrator.env]].args (train split, mode, executor backend, etc.)
# - [orchestrator.sampling] (max_tokens / temperature)

# 2-GPU layout: inference on GPU 0, trainer on GPU 1
inference_gpu_ids = [0]
trainer_gpu_ids = [1]

# Total number of optimizer steps.
# Start small for shakeout runs, then scale.
max_steps = 1000  # Full training run

[model]
# Qwen2.5-7B: 7B params, widely supported, good tool calling.
# Tried: gpt-oss-20b (vLLM bug), qwen3-vl-8b (VL not supported), mistral (not on HF)
name = "Qwen/Qwen2.5-7B-Instruct"

# 7B model doesn't need CPU offloading on 2x A6000

[wandb]
# Optional, but highly recommended for tracking.
project = "beautiful-soup-env"
name = "bs4-rl-qwen2.5-7b-lora"

[trainer.optim]
lr = 1e-5
weight_decay = 0.0

[trainer.model.experimental.lora]
# LoRA is the lowest-friction way to get a nice demo quickly.
rank = 8
alpha = 32
dropout = 0.0
target_modules = [
  "q_proj",
  "k_proj",
  "v_proj",
  "o_proj",
  "gate_proj",
  "up_proj",
  "down_proj"
]

[orchestrator]
# batch_size is "rollouts per batch" in prime-rl land.
# 7B model - can use larger batches on 2x A6000
batch_size = 128
rollouts_per_example = 8
seq_len = 8192
oversampling_factor = 2.0

mask_truncated_completions = false
zero_truncated_completions = true

[orchestrator.sampling]
# Per-rollout generation limit. Models need enough tokens to:
# - Include reasoning/chain-of-thought before tool calls
# - Generate Python code (BS4 snippets are typically 100-300 tokens)
# - Handle multi-turn conversations
# 768 was too tight - caused truncation before tool calls could be emitted.
# 10000 gives plenty of headroom for multi-turn tool calling during RL.
max_tokens = 10000

[orchestrator.buffer]
online_difficulty_filtering = true

[[orchestrator.env]]
# If you have uploaded to Hub, use "owner/name".
id = "seconds-0/beautiful-soup-env"

# Forwarded to your load_environment(**kwargs).
# This is where you set your splits/modes and the executor backend.
#
# For training runs:
# - split="train"
# - mode="tiered" is a good default for RL signal (overweights harder tasks)
# - executor_backend="prime" is the "professional" choice (sandboxed)
[orchestrator.env.args]
split = "train"
mode = "tiered"
difficulty = "mixed"
seed = 42
executor_backend = "prime"
network_access = true
timeout_s = 30.0
max_output_chars = 10000

[inference.model]
# Tool calling settings (important for your env because it relies on tools).
enable_auto_tool_choice = true
tool_call_parser = "hermes"
