# PRIME-RL single-file config for BeautifulSoup RL environment.
# Intended to run via:  uv run prime-rl @ configs/prime-rl/beautiful-soup-env.toml
#
# This follows the official "single TOML" prime-rl layout shown in Verifiers docs.
#
# Tune as needed:
# - [model].name (pick your base model)
# - [orchestrator] batch_size / rollouts_per_example (based on GPU + throughput)
# - [[orchestrator.env]].args (train split, mode, executor backend, etc.)
# - [orchestrator.sampling] (max_tokens / temperature)

# 8-GPU layout: inference on GPUs 0-3, trainer on GPUs 4-7
inference_gpu_ids = [0, 1, 2, 3]
trainer_gpu_ids = [4, 5, 6, 7]

# Total number of optimizer steps.
# Start small for shakeout runs, then scale.
max_steps = 1000  # Full training run

[model]
# Ministral-3-8B: 8B params, functional tool calls (63.2% baseline, 1148 calls).
# Selected because it's the ONLY benchmarked model that actually uses tools.
# Tried: gpt-oss-20b (vLLM bug), qwen2.5-7b (no OpenRouter tool support), qwen3-8b (0 tool calls)
name = "mistralai/Ministral-3-8B-Instruct-2512"

# 8B model fits on A6000 (24GB BF16, <12GB quantized)

[wandb]
# Optional, but highly recommended for tracking.
project = "beautiful-soup-env"
name = "bs4-rl-ministral-8b-lora"

[trainer.optim]
lr = 1e-5
weight_decay = 0.0

[trainer.model]
seq_len = 4096  # Must be >= orchestrator seq_len

[trainer.model.lora]
# LoRA is the lowest-friction way to get a nice demo quickly.
rank = 8
alpha = 32
dropout = 0.0
target_modules = [
  "q_proj",
  "k_proj",
  "v_proj",
  "o_proj",
  "gate_proj",
  "up_proj",
  "down_proj"
]

[orchestrator]
# batch_size is "rollouts per batch" in prime-rl land.
# 8B model on 8x A6000 - can use larger batches
batch_size = 256
rollouts_per_example = 8
seq_len = 4096  # Must be <= trainer model seq_len (default 2048, but we set trainer seq_len too)
oversampling_factor = 2.0
lora_name = "ministral-8b-bs4-lora"

[orchestrator.sampling]
# Per-rollout generation limit. Models need enough tokens to:
# - Include reasoning/chain-of-thought before tool calls
# - Generate Python code (BS4 snippets are typically 100-300 tokens)
# - Handle multi-turn conversations
# 768 was too tight - caused truncation before tool calls could be emitted.
# 10000 gives plenty of headroom for multi-turn tool calling during RL.
max_tokens = 10000

[orchestrator.buffer]
online_difficulty_filtering = true

[[orchestrator.env]]
# If you have uploaded to Hub, use "owner/name".
id = "seconds-0/beautiful-soup-env"

# Forwarded to your load_environment(**kwargs).
# This is where you set your splits/modes and the executor backend.
#
# For training runs:
# - split="train"
# - mode="tiered" is a good default for RL signal (overweights harder tasks)
# - executor_backend="prime" is the "professional" choice (sandboxed)
[orchestrator.env.args]
split = "train"
mode = "tiered"
difficulty = "mixed"
seed = 42
executor_backend = "prime"
network_access = true
timeout_s = 30.0
max_output_chars = 10000

[inference.model]
# Tool calling settings for Ministral (uses mistral parser, not hermes).
enable_auto_tool_choice = true
tool_call_parser = "mistral"
# Mistral requires special tokenizer/format settings
tokenizer_mode = "mistral"
config_format = "mistral"
load_format = "mistral"
# Disable CUDA graphs to avoid segfault in Ray workers (common vLLM issue)
enforce_eager = true
